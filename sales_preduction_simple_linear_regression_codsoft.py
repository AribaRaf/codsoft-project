# -*- coding: utf-8 -*-
"""sales preduction simple linear regression codsoft

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEG4MrJPYRwNNjCE-TqK-nkYQwd7aHe0

importing dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

"""data collection and analysis

loading dataset from csv file to pandas dataframe
"""

big_mart_data= pd.read_csv('/content/advertising.csv')

"""displaying the first five rows of the dataset"""

big_mart_data.head()

big_mart_data.shape

big_mart_data.info()

big_mart_data.describe()

big_mart_data= pd.read_csv('/content/advertising.csv')

big_mart_data.isnull().sum()

fig, axs = plt.subplots(3,figsize=(5,5))
plt1=sns.boxplot(big_mart_data['TV'], ax=axs[0])
plt2 = sns.boxplot(big_mart_data['Radio'],ax=axs[1])
plt3 = sns.boxplot(big_mart_data['Newspaper'],ax=axs[2])
plt.tight_layout()

plt.show(sns.boxplot(big_mart_data['Sales']))

sns.pairplot(big_mart_data, x_vars= ['TV','Newspaper' , 'Radio' ] , y_vars= 'Sales',  height= 4, aspect=1, kind= 'scatter')
plt.show()

sns.heatmap(big_mart_data.corr(), cmap='YlGnBu' , annot=True)
plt.show()

"""tv sales seem to be most correlated with the sales , take tv as the variable for regression ahead

model building

Equation of linear regression
y=c+m1x1+m2x2+...+mnxn

y
  is the response
c
  is the intercept
m1
  is the coefficient for the first feature
mn
  is the coefficient for the nth feature
In our case:

y=c+m1×TV

The  m
  values are called the model coefficients or model parameters
"""

x = big_mart_data['TV']
y = big_mart_data['Sales']

"""test train split"""

x_train, x_test , y_train , y_test = train_test_split(x , y , train_size=0.7 , test_size= 0.3, random_state= 100)

x_train.head()

y_train.head()

"""adding consrant as an intercept is needed for a linear regression model"""

x_train_sm= sm.add_constant(x_train)

"""fitting the regression line using ols"""

lr= sm.OLS(y_train, x_train_sm).fit()

"""printing the parameters i.e., the intercept and the slope of the regression"""

lr.params

print(lr.summary())

""" The coefficient for TV is 0.054, with a very low p value¶
The coefficient is statistically significant. So the association is not purely by chance.

2. R - squared is 0.816
Meaning that 81.6% of the variance in Sales is explained by TV

This is a decent R-squared value.

3. F statistic has a very low p value (practically low)
Meaning that the model fit is statistically significant, and the explained variance isn't purely by chance.

Sales=6.948+0.054×TV


"""

plt.scatter(x_train , y_train)
plt.plot(x_train, 6.948 + 0.054*x_train, 'r')
plt.show()

"""model evaluation"""

y_train_pred = lr.predict(x_train_sm)
res = (y_train - y_train_pred)
fig = plt.figure()
sns.distplot(res, bins=15)
fig.suptitle('Error terms', fontsize=15)
plt.xlabel('y_train - y_train_pred', fontsize=15)
plt.show()

plt.scatter(x_train,res)
plt.show()

"""predicting to the test set

add a constant to the xtest set
"""

x_test_sm = sm.add_constant(x_test)

y_pred = lr.predict(x_test_sm)
y_pred.head()

"""looking for the rmse

```
`# This is formatted as code`
```


"""

np.sqrt(mean_squared_error(y_test, y_pred))
np.sqrt(mean_squared_error(y_test, y_pred))

r_square=r2_score(y_test, y_pred)
r_square

"""visualizing the fit on the test set"""

plt.scatter(x_test,y_test)
plt.plot(x_test,6.948 + 0.054 * x_test, 'r')

"""this depicts that the model we built perfectly predicts the target variables ."""